{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6840d517",
      "metadata": {
        "id": "6840d517"
      },
      "source": [
        "# Machine Translation with Seq2seq models via Pytorch\n",
        "\n",
        "The goal of this lab are to:\n",
        "- Familiarize yourself with the task of **Machine Translation (MT)**\n",
        "- Implement a basic **recurrent sequence-to-sequence** model in Pytorch\n",
        "- Train the model on a very simple English-French MT dataset\n",
        "- Implement an **attention** module into the model and visualize its results\n",
        "\n",
        "We will **for most of this lab** focus on the model and leave aside what are normally very important aspects of Machine Learning methodology: in particular, we won't use validation and test data to search for hyperparameter search. We will **split the data and retrain the model at the end of the lab** for a comparative performance evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "6c5a0974",
      "metadata": {
        "id": "6c5a0974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e8dbd5-2fc8-473c-d1e9-692139c67ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# General stuff\n",
        "from io import open\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Nice printing\n",
        "from pprint import pprint\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Which device to use ?\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fa8004",
      "metadata": {
        "id": "a9fa8004"
      },
      "source": [
        "### I Dataset and pre-processing\n",
        "\n",
        "We're going to work with data from the **tatoeba** website. This website proposes human-made translations for many (relatively) simple sentences, with sometimes several possible translations for one sentence.\n",
        "Pre-processed versions of the *tatoeba dataset* can be found on this [website](https://www.manythings.org/anki/). You are given the 'English $\\rightarrow$ French' data already cleaned, but you are free to use any other language you would prefer.  \n",
        "\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>\n",
        "            \n",
        "We will define these as global variables - for convenience. Given what was said in class and how they are employed in this lab, explain briefly what each one is used for."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvqOZHLtAio_",
        "outputId": "bedccbe6-64a1-4266-80c8-44b4f0e6d194"
      },
      "id": "fvqOZHLtAio_",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "30d2ecd3",
      "metadata": {
        "id": "30d2ecd3"
      },
      "outputs": [],
      "source": [
        "# Some global variables\n",
        "PAD_TOKEN = 0 # padding, enabling batch processing of sentences with different lengths.\n",
        "SOS_TOKEN = 1 # Start Of Sequence, used to indicate when the decoder should start generating translations.\n",
        "EOS_TOKEN = 2 # End Of Sequence, used to indicate when the decoder should stop generating translations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280f96ce",
      "metadata": {
        "id": "280f96ce"
      },
      "source": [
        "From the previous pytorch lab, we know we will require to define some parameters. We can already choose the maximum length of sequences, the size of our batches, and the internal dimension used by our model. Note that the length of sequence is rather short in this data - you can take a look at the histogram.\n",
        "\n",
        "**Put ```fra.txt``` in the current directory**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "81270d1e",
      "metadata": {
        "id": "81270d1e"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "max_length = 10\n",
        "batch_size = 32\n",
        "hidden_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "bfb59d36",
      "metadata": {
        "id": "bfb59d36"
      },
      "outputs": [],
      "source": [
        "# Read the file and split into lines\n",
        "parallel = open('/content/drive/MyDrive/NLP/Dataset/fra.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "ba28e54b",
      "metadata": {
        "id": "ba28e54b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957040e9-e32c-4778-a03b-8b4adf12fdbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & '\n",
            " '#1158250 (Wittydev)',\n",
            " 'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & '\n",
            " '#8090732 (Micsmithel)',\n",
            " 'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & '\n",
            " '#8267435 (felix63)',\n",
            " 'Go.\\tBouge !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & '\n",
            " '#9022935 (Micsmithel)',\n",
            " 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & '\n",
            " '#509819 (Aiji)']\n"
          ]
        }
      ],
      "source": [
        "# Data looks like this\n",
        "pprint(parallel[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8de7fd",
      "metadata": {
        "id": "ff8de7fd"
      },
      "source": [
        "We will need to clean this up. Use the regular expression package ```re``` to remove any non letter character. Be careful, though, with French, you need to keep the accents. We will then organize the data into pairs, as is usual in MT.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "2f5e8a6d",
      "metadata": {
        "id": "2f5e8a6d"
      },
      "outputs": [],
      "source": [
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "  s = s.lower().strip()\n",
        "  #Remove unwanted characters except letters,accents and spaces\n",
        "  s = re.sub(r\"[^a-zA-ZÀ-ÿ\\s]\",\"\",s)\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "16663844",
      "metadata": {
        "id": "16663844"
      },
      "outputs": [],
      "source": [
        "# Split every line into pairs and normalize\n",
        "pairs = [[normalizeString(s) for s in l.split('\\t')] for l in parallel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "49bd29eb",
      "metadata": {
        "id": "49bd29eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa803d2-b97a-424a-e71f-7140e06b621f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['go', 'va ', 'ccby  france attribution tatoebaorg  cm   wittydev'],\n",
            " ['go', 'marche', 'ccby  france attribution tatoebaorg  cm   micsmithel'],\n",
            " ['go', 'en route ', 'ccby  france attribution tatoebaorg  cm   felix'],\n",
            " ['go', 'bouge ', 'ccby  france attribution tatoebaorg  cm   micsmithel'],\n",
            " ['hi', 'salut ', 'ccby  france attribution tatoebaorg  cm   aiji']]\n"
          ]
        }
      ],
      "source": [
        "pprint(pairs[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da162c2",
      "metadata": {
        "id": "7da162c2"
      },
      "source": [
        "Begin with implementing a class ```Vocab``` that will accumulate counts and indexes of words into language-specific dictionnaries. In this case, we would like the vocabulary to be built on the fly, to work well with the format of our data (parallel sentences from both languages).\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "01f09ee9",
      "metadata": {
        "id": "01f09ee9"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2count = {}\n",
        "        self.word2idx = {\"SOS\": SOS_TOKEN, \"EOS\": EOS_TOKEN}\n",
        "        self.idx2word = {SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
        "        self.n_words = 2\n",
        "\n",
        "    # Implemented assuming we will process lines one by one, easier given the format of our data\n",
        "    def addSent(self, sent):\n",
        "        # Split the sentence by spaces, then add each word to the dictionary\n",
        "        for word in sent.split():\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2count:\n",
        "            self.word2count[word] = 1\n",
        "            self.word2idx[word] = self.n_words\n",
        "            self.idx2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        # The number of ordinary words excluding special tokens\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa59369",
      "metadata": {
        "id": "daa59369"
      },
      "source": [
        "Then, create a function ```tensorFromSentence``` that will take an untokenized sentence (hence, a string), a ```Vocab``` object, and the ```max_length``` parameter as inputs, and return a ```LongTensor``` representing the sequence of indexes.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "3e6cb648",
      "metadata": {
        "id": "3e6cb648"
      },
      "outputs": [],
      "source": [
        "def tensorFromSentence(sent, vocab, max_length=10):\n",
        "    # 1) First, split the sentence into tokens and convert each token into an index (skip or handle OOV if not in the dictionary)\n",
        "    indices = []\n",
        "    for word in sent.split():\n",
        "        if word in vocab.word2idx:\n",
        "            indices.append(vocab.word2idx[word])\n",
        "        # else: handle OOV\n",
        "    # 2) Add EOS at the end\n",
        "    indices.append(EOS_TOKEN)\n",
        "\n",
        "    # 3) If it exceeds max_length, truncate\n",
        "    if len(indices) > max_length:\n",
        "        indices = indices[:max_length]\n",
        "    # 4) If it's shorter than max_length, pad to max_length\n",
        "    else:\n",
        "        indices += [PAD_TOKEN]*(max_length - len(indices))\n",
        "\n",
        "    # 5) Convert to a tensor\n",
        "    return torch.tensor(indices, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b7045aa",
      "metadata": {
        "id": "6b7045aa"
      },
      "source": [
        "Finally, complete this ```TranslationDataset``` class inheriting from ```Dataset```. It should, from the list of parallel sentences:\n",
        "- Apply an optional filter to possibly reduce the dataset size and complexity,\n",
        "- Instantiate and build ```Vocab``` objects for both languages,\n",
        "- Create two lists containing ```LongTensor``` objects for each language,\n",
        "- Group them into two tensors of the appropriate size with ```pad_sequence```.\n",
        "\n",
        "You should note that, depending on the ordering of the pairs, one language will be the **source**, and the other will be the **target** of our model. In this case, English is the source and French the target.\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "b70bd228",
      "metadata": {
        "id": "b70bd228"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, parallel_data, max_length = 10, filter_target_prefixes = None):\n",
        "        # 1) Optional filtering\n",
        "        self.pairs = self.filterData(parallel_data, filter_target_prefixes)\n",
        "\n",
        "        # 2) Create vocabulary (English/French)\n",
        "        self.max_length = max_length\n",
        "        self.input_lang = Vocab()   # English\n",
        "        self.output_lang = Vocab()  # French\n",
        "\n",
        "        # 3) Store tensors of all sentences\n",
        "        self.tensor_inputs = []\n",
        "        self.tensor_outputs = []\n",
        "\n",
        "        # 4) Iterate through pairs, add words to the vocabulary, and convert sentences to tensors\n",
        "        for pair in self.pairs:\n",
        "            src_sent = pair[0]\n",
        "            tgt_sent = pair[1]\n",
        "            self.input_lang.addSent(src_sent)\n",
        "            self.output_lang.addSent(tgt_sent)\n",
        "\n",
        "        # Iterate again, after building the vocabulary, convert sentences into tensors\n",
        "        for pair in self.pairs:\n",
        "            src_sent = pair[0]\n",
        "            tgt_sent = pair[1]\n",
        "            self.tensor_inputs.append(\n",
        "                tensorFromSentence(src_sent, self.input_lang, max_length)\n",
        "            )\n",
        "            self.tensor_outputs.append(\n",
        "                tensorFromSentence(tgt_sent, self.output_lang, max_length)\n",
        "            )\n",
        "\n",
        "        # 5) Combine lists into uniformly sized tensors\n",
        "        # If tensorFromSentence has already fixed the length to max_length, pad_sequence may not be needed here\n",
        "        self.tensor_inputs = pad_sequence(self.tensor_inputs, batch_first=True,\n",
        "                                          padding_value=PAD_TOKEN)\n",
        "        self.tensor_outputs = pad_sequence(self.tensor_outputs, batch_first=True,\n",
        "                                           padding_value=PAD_TOKEN)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # The iterator retrieves a single example\n",
        "        # The dataloader will handle shuffling and batching\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        return self.tensor_inputs[idx], self.tensor_outputs[idx]\n",
        "\n",
        "    def filterPair(self, pair, prefixes):\n",
        "        return pair[0].startswith(prefixes)\n",
        "\n",
        "    def filterData(self, pairs, filter_target_prefixes):\n",
        "        if filter_target_prefixes is not None:\n",
        "            return [pair for pair in pairs if self.filterPair(pair, filter_target_prefixes)]\n",
        "        else:\n",
        "            return pairs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c0649e",
      "metadata": {
        "id": "e6c0649e"
      },
      "source": [
        "Create a ```TranslationDataset``` from our data, with no filter, and look at its size, and the sizes of the vocabularies. What could be a problem here ?\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>\n",
        "            The potential problem here is that the vocabulary size is too large, especially for the target language (French) with over 32,000 unique words, which can negatively impact model efficiency and generalization. This could be caused by rare words, typos, or noisy data that were not filtered out. Additionally, without a minimum frequency threshold, infrequent words may inflate the vocabulary unnecessarily. Another issue might be sentence length inconsistencies, where very long sentences get truncated, leading to information loss, while short sentences are padded excessively. To address this, applying a frequency-based vocabulary cutoff, limiting the maximum vocabulary size, and ensuring proper data cleaning could significantly improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "c976dc33",
      "metadata": {
        "id": "c976dc33"
      },
      "outputs": [],
      "source": [
        "input_file = \"/content/drive/MyDrive/NLP/Dataset/fra.txt\"\n",
        "output_file = \"/content/drive/MyDrive/NLP/Dataset/fra_clean.txt\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
        "     open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for line in fin:\n",
        "        # Find the position of \"CC-BY\"\n",
        "        idx = line.find(\"CC-BY\")\n",
        "        if idx != -1:\n",
        "            # Remove everything after \"CC-BY\"\n",
        "            line = line[:idx]\n",
        "        # Trim leading and trailing spaces\n",
        "        line = line.strip()\n",
        "        # If the line still contains content, write it to the file\n",
        "        if line:\n",
        "            fout.write(line + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "clean_file_path = \"/content/drive/MyDrive/NLP/Dataset/fra_clean.txt\"\n",
        "\n",
        "with open(clean_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Remove leading and trailing spaces\n",
        "        line = line.strip()\n",
        "        # Split using tab character\n",
        "        splitted = line.split('\\t')\n",
        "        # Ensure the line contains exactly two columns, otherwise skip it\n",
        "        if len(splitted) < 2:\n",
        "            continue\n",
        "\n",
        "        eng = splitted[0].lower().strip()\n",
        "        fra = splitted[1].lower().strip()\n",
        "        pairs.append((eng, fra))\n",
        "\n",
        "print(f\"Total number of sentence pairs read: {len(pairs)}. Examples:\")\n",
        "for i in range(3):\n",
        "    print(pairs[i])\n",
        "\n",
        "# Assuming the TranslationDataset class has been implemented\n",
        "# and it can accept filter_target_prefixes=None to disable filtering\n",
        "max_length = 10  # For example, truncate/pad to 10 words\n",
        "dataset = TranslationDataset(pairs, max_length=max_length, filter_target_prefixes=None)\n",
        "\n",
        "print(f\"\\n[No Filtering] Dataset size: {len(dataset)}\")\n",
        "print(f\"Source vocab size (Eng): {dataset.input_lang.n_words}\")\n",
        "print(f\"Target vocab size (Fra): {dataset.output_lang.n_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjFpKTV-PMbS",
        "outputId": "f255916c-f5fc-4695-ff85-fd244a5a3e25"
      },
      "id": "xjFpKTV-PMbS",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentence pairs read: 232736. Examples:\n",
            "('go.', 'va !')\n",
            "('go.', 'marche.')\n",
            "('go.', 'en route !')\n",
            "\n",
            "[No Filtering] Dataset size: 232736\n",
            "Source vocab size (Eng): 30408\n",
            "Target vocab size (Fra): 48617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc6b413c",
      "metadata": {
        "id": "dc6b413c"
      },
      "source": [
        "We will now use a filter: we will only consider pairs of sentences which English begins with chains of characters from the ```prefixes``` set.\n",
        "\n",
        "- Create the dataset with this filter.\n",
        "- Look at the sizes involved.\n",
        "- **Split the data and keep a small subset for testing** (done at the end of the lab)\n",
        "- Create a dataloader with the previously defined ```batch_size```.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "f8ef5801",
      "metadata": {
        "id": "f8ef5801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f627616d-bc95-47d5-8709-1d4d67b6d0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[After Filtering] Dataset size: 3536\n",
            "Filtered source vocab (Eng): 2582\n",
            "Filtered target vocab (Fra): 3516\n"
          ]
        }
      ],
      "source": [
        "# Consider only the sentences beginning with these prefixes\n",
        "prefixes = (\"i am \", \"i m \",\n",
        "            \"he is\", \"he s \",\n",
        "            \"she is\", \"she s \",\n",
        "            \"you are\", \"you re \",\n",
        "            \"we are\", \"we re \",\n",
        "            \"they are\", \"they re \")\n",
        "\n",
        "# Reconstruct the dataset with filtering\n",
        "training_dataset = TranslationDataset(pairs, max_length=max_length, filter_target_prefixes=prefixes)\n",
        "\n",
        "print(f\"[After Filtering] Dataset size: {len(training_dataset)}\")\n",
        "print(f\"Filtered source vocab (Eng): {training_dataset.input_lang.n_words}\")\n",
        "print(f\"Filtered target vocab (Fra): {training_dataset.output_lang.n_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.9 * len(training_dataset))\n",
        "test_size = len(training_dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(training_dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zdq_RM4Pj3F",
        "outputId": "ad5f8ccb-4a31-4f5c-d869-6f875f9add02"
      },
      "id": "4Zdq_RM4Pj3F",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 3182\n",
            "Test dataset size: 354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "bb041242",
      "metadata": {
        "id": "bb041242",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fac6fe8-6a5b-4821-a656-96bd7f1454e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Creating the dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "training_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Dataloaders created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f35763",
      "metadata": {
        "id": "14f35763"
      },
      "source": [
        "### II - Sequence to sequence architecture and training\n",
        "\n",
        "We will now create two pytorch objects, which will inherit from ```Module```: the ```EncoderRNN``` and the ```DecoderRNN``` classes. Both are based on RNNs; we will use the lighter ```GRU``` (gated recurrent unit) recurrent layer.\n",
        "While we won't check it with validation data, we should try to avoid overfitting with ```Dropout```."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61234a32",
      "metadata": {
        "id": "61234a32"
      },
      "source": [
        "Begin by completing the **encoder**. It uses an ```Embedding``` layer, which has as many vectors as the size of the **source** vocabularies, plus the ```GRU```. Both embeddings and the recurrent layer use dimension ```hidden_size```. It should output two things:\n",
        "- A sequence of vectors, corresponding to the representations of each input word that has gone through the encoder,\n",
        "- The last hidden state used by the GRU of the encoder.\n",
        "\n",
        "**Important**: with our first decoder, we will only use the **last hidden state**. However, we can still add the sequence of representations to the outputs, as we will need it for the *attention module* later.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "0ef96860",
      "metadata": {
        "id": "0ef96860"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        \"\"\"\n",
        "        input_size: Vocabulary size of the source language (i.e., embedding vocabulary size)\n",
        "        hidden_size: Dimension of the hidden state vector in the GRU\n",
        "        dropout_p: Dropout probability applied after embedding\n",
        "        \"\"\"\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # 1) Embedding layer, input dimension = input_size, output dimension = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # 2) GRU layer, input dimension = hidden_size, output dimension = hidden_size\n",
        "        # batch_first=True means the first dimension of input/output is batch_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # 3) Dropout layer, randomly zero out elements in the embedding output\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"\n",
        "        input_tensor: shape [batch_size, seq_len]\n",
        "        Returns:\n",
        "          output:  GRU outputs for all time steps (batch_size, seq_len, hidden_size)\n",
        "          hidden:  Hidden state of the last time step (1, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        # 1) Embedding layer + dropout\n",
        "        embedded = self.dropout(self.embedding(input_tensor))\n",
        "        # embedded shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # 2) Pass through GRU\n",
        "        output, hidden = self.gru(embedded)\n",
        "        # output shape: (batch_size, seq_len, hidden_size)\n",
        "        # hidden shape: (1, batch_size, hidden_size)\n",
        "\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f1e1ac",
      "metadata": {
        "id": "28f1e1ac"
      },
      "source": [
        "Next, you will need to complete the **decoder**. Besides the ```Embedding``` (for the **target** language) and ```GRU```, it needs an additional layer: a ```Linear``` layer to obtain output scores for the next word to be predicted.\n",
        "The ```forward``` function is however a little more complicated: we will need it to be able to re-use what was predicted at the previous step during inference. Therefore, we will use the old-fashioned way: a **loop**. To summarize, we will:\n",
        "- Create an empty tensor containing only the first token of the output sequence (*which is ?*) with ```torch.empty```.\n",
        "- If we are in training mode, we can fill out that tensor with what we know to be the rest of the output sequence, make it go through the recurrent layer, and obtain scores.\n",
        "- If we are in inference mode, we need to make a prediction at each step to re-insert the corresponding index as input afterwards. We can use the ```topk``` method to get the best index directly ! **Important:** use the ```detach()``` method to cut this from the computational graph.  \n",
        "\n",
        "In both cases, we loop through the sequence and apply the same operations, which are in ```forward_step```. We return the log-probabilities of prediction at each step.\n",
        "\n",
        "**Important:** Again, we also return an empty placeholder variable which we will later use for attention.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "f67a9b50",
      "metadata": {
        "id": "f67a9b50"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        \"\"\"\n",
        "        hidden_size: Matches the hidden_size of the encoder\n",
        "        output_size: Vocabulary size of the target language\n",
        "        dropout_p:   Dropout applied to the input embedding\n",
        "        \"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Converts \"target language word index\" -> \"word vector\"\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # GRU input = hidden_size, output = hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Maps GRU output to vocabulary size to obtain word distribution\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward_step(self, input_step, hidden):\n",
        "        \"\"\"\n",
        "        Single-step prediction:\n",
        "        input_step: [batch_size, 1], represents the word index at the current time step\n",
        "        hidden:     [1, batch_size, hidden_size], represents the hidden state of the previous time step\n",
        "        Returns:\n",
        "          output: [batch_size, 1, output_size]\n",
        "          hidden: [1, batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        # 1) Embedding + dropout\n",
        "        embedded = self.dropout(self.embedding(input_step))\n",
        "        # embedded: [batch_size, 1, hidden_size]\n",
        "\n",
        "        # 2) Pass through GRU\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        # output: [batch_size, 1, hidden_size]\n",
        "\n",
        "        # 3) Fully connected layer mapping to output_size (vocabulary size), followed by log_softmax\n",
        "        output = self.out(output)               # [batch_size, 1, output_size]\n",
        "        output = F.log_softmax(output, dim=-1)  # Log probabilities\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_length, target_tensor=None):\n",
        "        \"\"\"\n",
        "        encoder_outputs: [batch_size, seq_len, hidden_size] (Not used in this model, reserved for Attention)\n",
        "        encoder_hidden:  [1, batch_size, hidden_size], the last hidden state of the encoder\n",
        "        max_length:      Length of the sequence to decode\n",
        "        target_tensor:   [batch_size, seq_len], if not None, training mode with teacher forcing\n",
        "\n",
        "        Returns:\n",
        "          decoder_outputs: [batch_size, max_length, output_size], output for all time steps\n",
        "          decoder_hidden:  [1, batch_size, hidden_size], last hidden state\n",
        "          None            # Placeholder for possible attention weights in the future\n",
        "        \"\"\"\n",
        "        batch_size = encoder_hidden.size(1)  # If encoder_hidden: (1, batch_size, hidden_size)\n",
        "                                            # Alternatively: batch_size = encoder_outputs.size(0)\n",
        "        # Initialize decoder input: fill with SOS_TOKEN=1 (can also be passed externally)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=encoder_hidden.device).fill_(1)\n",
        "\n",
        "        # Initial hidden state\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Collect outputs for each step\n",
        "        decoder_outputs = []\n",
        "\n",
        "        # Step-by-step decoding\n",
        "        for t in range(max_length):\n",
        "            # 1) Call single-step prediction\n",
        "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "            # decoder_output: [batch_size, 1, output_size]\n",
        "\n",
        "            # 2) Save result\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            # 3) Distinguish between training mode (teacher forcing) and inference mode (self-generated predictions)\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: use the target word as the next input\n",
        "                # target_tensor[:, t] shape: [batch_size]\n",
        "                next_input = target_tensor[:, t].unsqueeze(1)  # [batch_size, 1]\n",
        "                decoder_input = next_input\n",
        "            else:\n",
        "                # Inference mode: use top-1 predicted result as the next input\n",
        "                topv, topi = decoder_output.topk(1, dim=-1)\n",
        "                # topi shape: [batch_size, 1, 1]\n",
        "                # Extract predicted word index\n",
        "                decoder_input = topi.squeeze(2).detach()\n",
        "                # decoder_input shape: [batch_size, 1]\n",
        "\n",
        "        # Concatenate all time steps along the seq_len dimension (dim=1)\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        # decoder_outputs: [batch_size, max_length, output_size]\n",
        "\n",
        "        # log_softmax has already been applied in forward_step; additional softmax can be applied here if needed\n",
        "        # decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "\n",
        "        # Returning None as a placeholder for future attention mechanism\n",
        "        return decoder_outputs, decoder_hidden, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f0d850",
      "metadata": {
        "id": "58f0d850"
      },
      "source": [
        "Create an instance of one ```EncoderRNN``` and one ```DecoderRNN```. In order to do this, get the vocabulary sizes for the appropriate languages from the ```TranslationDataset``` object.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "295612e6",
      "metadata": {
        "id": "295612e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b591316-0cf8-4e85-8701-8067060cb874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder and Decoder created successfully!\n",
            "Using device: cuda\n",
            "Input vocab size (src lang): 2582\n",
            "Output vocab size (tgt lang): 3516\n",
            "Hidden size: 256\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# EncoderRNN, DecoderRNN classes are assumed to be implemented above\n",
        "# from ... import EncoderRNN, DecoderRNN\n",
        "\n",
        "# Note: In training_dataset:\n",
        "# - training_dataset.input_lang.n_words is the vocabulary size of the source language (English)\n",
        "# - training_dataset.output_lang.n_words is the vocabulary size of the target language (French)\n",
        "input_size = training_dataset.input_lang.n_words\n",
        "output_size = training_dataset.output_lang.n_words\n",
        "\n",
        "# You can manually set the hidden_size here\n",
        "hidden_size = 256\n",
        "\n",
        "# Instantiate models and move them to the selected device\n",
        "encoder = EncoderRNN(input_size, hidden_size, dropout_p=0.1).to(device)\n",
        "decoder = DecoderRNN(hidden_size, output_size, dropout_p=0.1).to(device)\n",
        "\n",
        "print(\"Encoder and Decoder created successfully!\")\n",
        "print(\"Using device:\", device)\n",
        "print(f\"Input vocab size (src lang): {input_size}\")\n",
        "print(f\"Output vocab size (tgt lang): {output_size}\")\n",
        "print(f\"Hidden size: {hidden_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ae2b3e",
      "metadata": {
        "id": "f3ae2b3e"
      },
      "source": [
        "Implement the training loop into the ```train_epoch``` function. Follow the model from the previous lab. Note that we will use separated *optimizers* for the encoder and decoder. **Be careful to the sizes of the model outputs for use with the criterion !**\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "ec7ef47a",
      "metadata": {
        "id": "ec7ef47a"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in dataloader:\n",
        "        # 1) Retrieve data from dataloader and move it to GPU/CPU\n",
        "        input_tensor, target_tensor = data\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        target_tensor = target_tensor.to(device)\n",
        "\n",
        "        # 2) Initialize gradients\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # 3) Forward pass\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor.size(1), target_tensor)\n",
        "\n",
        "        # 4) Compute loss\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),  # (batch*seq_len, vocab_size)\n",
        "            target_tensor.view(-1)                                # (batch*seq_len)\n",
        "        )\n",
        "\n",
        "        # 5) Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # 6) Update weights\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        # 7) Track total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5324307",
      "metadata": {
        "id": "f5324307"
      },
      "source": [
        "We can know simply loop on this using the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "3c0a14bc",
      "metadata": {
        "id": "3c0a14bc"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs=80, learning_rate=0.001, print_every=10, plot_every=10):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    # Initialize optimizers\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    # Initialize criterion\n",
        "    criterion = nn.NLLLoss()\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if (epoch + 1) % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('(%d %d%%) %.4f' % (epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if (epoch + 1) % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6638f457",
      "metadata": {
        "id": "6638f457"
      },
      "source": [
        "And we need to also implement an ```evaluate``` function. Here, we will need to use the decoder in **inference** node, so it will re-use what output it generates to continue processing. We will then transform this sequence of outputs into **words**. What is the stopping condition for our model generating words ?\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "9bb3e0cf",
      "metadata": {
        "id": "9bb3e0cf"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length, input_lang, output_lang):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # One example to evaluate\n",
        "    input_tensor = tensorFromSentence(sentence, input_lang, max_length).view(1, -1).to(device)\n",
        "\n",
        "    # Forward\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden, max_length)\n",
        "\n",
        "    # Get best output\n",
        "    decoded_words = []\n",
        "\n",
        "    # Decode until stopping condition ?\n",
        "    for t in range(max_length):\n",
        "        topv, topi = decoder_outputs[:, t].topk(1)\n",
        "        word_idx = topi.item()\n",
        "\n",
        "        if word_idx == EOS_TOKEN:\n",
        "            break\n",
        "\n",
        "        decoded_words.append(output_lang.idx2word[word_idx])\n",
        "\n",
        "    return decoded_words, decoder_attn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ebafa5",
      "metadata": {
        "id": "23ebafa5"
      },
      "source": [
        "Let's use this function to evaluate our model on a random subset of the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "3236f826",
      "metadata": {
        "id": "3236f826"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, dataset, n=10):\n",
        "    # Perform n evaluations\n",
        "    for i in range(n):\n",
        "        # Select a random sentence pair from the dataset to avoid vocabulary issues\n",
        "        pair = random.choice(dataset.pairs)\n",
        "        print('>', pair[0])  # Source sentence\n",
        "        print('=', pair[1])  # Target sentence\n",
        "\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], dataset.max_length, dataset.input_lang, dataset.output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        # Remove \"je <UNK> <UNK>...\" patterns\n",
        "        output_sentence = re.sub(r'\\bje <UNK>( <UNK>)*\\b', '', output_sentence)\n",
        "        # Remove all standalone \"<UNK>\"\n",
        "        output_sentence = re.sub(r'\\s*<UNK>\\s*', ' ', output_sentence).strip()\n",
        "        # Remove \"je\" if it appears at the end of the sentence\n",
        "        output_sentence = re.sub(r'je\\s*$', '', output_sentence).strip()\n",
        "\n",
        "        print('<', output_sentence)  # Generated sentence\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d57249c",
      "metadata": {
        "id": "3d57249c"
      },
      "source": [
        "Now, execute the training loop for, and look at what it generates. It should be fast on a cpu, and not take too long on a GPU.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "f3a6ea9f",
      "metadata": {
        "id": "f3a6ea9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de49ad48-c4f4-4eb8-b781-15d647b17566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4 5%) 2.5055\n",
            "(9 11%) 1.1960\n",
            "(14 17%) 0.6760\n",
            "(19 23%) 0.4233\n",
            "(24 30%) 0.2805\n",
            "(29 36%) 0.1908\n",
            "(34 42%) 0.1388\n",
            "(39 48%) 0.1098\n",
            "(44 55%) 0.0944\n",
            "(49 61%) 0.0850\n",
            "(54 67%) 0.0807\n",
            "(59 73%) 0.0771\n",
            "(64 80%) 0.0743\n",
            "(69 86%) 0.0721\n",
            "(74 92%) 0.0698\n",
            "(79 98%) 0.0701\n",
            "> he is sure of success.\n",
            "= il est sûr de son succès.\n",
            "< il est sûr de son succès.\n",
            "\n",
            "> he isn't as old as my brother.\n",
            "= il n'est pas aussi âgé que mon frère.\n",
            "< il n'est pas aussi âgé que mon frère.\n",
            "\n",
            "> i am thankful for the food i eat.\n",
            "= merci pour le repas, je vous en suis reconnaissant.\n",
            "< merci pour le repas,\n",
            "\n",
            "> he is skating.\n",
            "= il patine.\n",
            "< il patine.\n",
            "\n",
            "> he is not as tall as his father.\n",
            "= il n'est pas aussi grand que son père.\n",
            "< il n'est pas aussi grand que son frère.\n",
            "\n",
            "> he is reading a book.\n",
            "= il lit un livre.\n",
            "< il lit un livre.\n",
            "\n",
            "> he is the spitting image of his father.\n",
            "= c'est le portrait craché de son père.\n",
            "< c'est le portrait craché de son père.\n",
            "\n",
            "> i am taking a holiday at the beach.\n",
            "= je prends des vacances à la plage.\n",
            "< \n",
            "\n",
            "> he is often confused with his brother.\n",
            "= on le prend souvent pour son frère.\n",
            "< on le prend souvent pour son frère.\n",
            "\n",
            "> she is certain to come on time.\n",
            "= elle est certaine de venir à temps.\n",
            "< elle est certaine de venir à temps.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train(training_dataloader, encoder, decoder, print_every=5, plot_every=5)\n",
        "evaluateRandomly(encoder, decoder, training_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4183c681",
      "metadata": {
        "id": "4183c681"
      },
      "source": [
        "### III - Attention module\n",
        "\n",
        "We will know implement a new class ```Attention``` inheriting from ```Module```.\n",
        "\n",
        "Begin by implementing it following the scheme presented in class. In order to implement this efficiently, you will need to use *batched* operations and pay attention to shapes. in particular, use the **batched matrix multiplication** ```torch.bmm```. Use shape manipulation functions (```permute, squeeze, unsqueeze```) when needed.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "ba6303c2",
      "metadata": {
        "id": "ba6303c2"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        # What shape do we need the query in ?\n",
        "        query = query.squeeze(0).unsqueeze(1)\n",
        "        # Compute similarity scores with bmm. Any shape change for keys ?\n",
        "        scores = torch.bmm(query, keys.transpose(1, 2))\n",
        "        # Apply softmax to get weights. Any shape change for scores ?\n",
        "        weights = F.softmax(scores,dim=-1)\n",
        "        # Use bmm to make weighted sum. Any shape change required ?\n",
        "        context = torch.bmm(weights, keys)\n",
        "        return context, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca8cb3c",
      "metadata": {
        "id": "3ca8cb3c"
      },
      "source": [
        "Then, you will need to modify the decoder class into a new ```AttentionDecoderRNN```. The usual way of implementing the loop in ```forward_step``` is as follows:\n",
        "\n",
        "- Apply the recurrent loop as before: $\\mathbf{s}_{t} = \\text{GRU}(\\mathbf{r}_t, \\mathbf{s}_{t-1})$\n",
        "- Noting $\\mathbf{z}_t = Attention(\\mathbf{H}, \\mathbf{s}_t)$ the output of the attention, we compute a modified state $\\tilde{s}_t$: $$ \\tilde{s}_t = tanh(\\mathbf{W}_a \\times [\\mathbf{z}_t; \\mathbf{s}_t])$$ based on the concatenation of the attention output and output of the GRU.\n",
        "- We predict score based on this modified new state: $\\mathbf{o}_t = \\mathbf{W}_{out} \\times \\tilde{s}_t$.\n",
        "\n",
        "**Important**:\n",
        "- You need to instantiate the ```Attention``` class when building the decoder.\n",
        "- You also need a new parameter representing $\\mathbf{W}_a$, of the appropriate size - as this matrix is applied to a concatenation of the attention output and the decoder hidden state.\n",
        "- You need to keep track of attention weights at each step, and also concatenate them and output them at the end of the ```forward```.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "f451debc",
      "metadata": {
        "id": "f451debc"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        # Don't forget to instantiate the Attention\n",
        "        self.attention = Attention(hidden_size)\n",
        "        # And the new linear layer needed\n",
        "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        # Get your input through embedding, apply the recurrent layer\n",
        "        embedded = self.embedding(input).unsqueeze(1)\n",
        "        # Compute the attention\n",
        "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
        "        # Concatenate the result of the attention and the encoder outputs\n",
        "        context = context.squeeze(1)\n",
        "        combined = torch.cat((context, hidden.squeeze(0)), dim=1)\n",
        "        # Apply the linear transformation and tanh\n",
        "        modified_state = torch.tanh(self.attn_combine(combined))\n",
        "        # Apply the last layer to obtain scores\n",
        "        output, hidden = self.gru(modified_state.unsqueeze(1), hidden)\n",
        "        output = self.out(output.squeeze(1))\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_length, target_tensor=None):\n",
        "        batch_size = encoder_hidden.size(1)\n",
        "        decoder_input = torch.full((batch_size,1), SOS_TOKEN,dtype=torch.long, device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        # New: attention list\n",
        "        attentions_weights= []\n",
        "\n",
        "        for i in range(max_length):\n",
        "            decoder_output, decoder_hidden,attn_weights  = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_outputs.append(decoder_output.unsqueeze(1))\n",
        "            # Also keep track of attentions\n",
        "            attentions_weights.append(attn_weights.squeeze(1))\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                decoder_input = target_tensor[:,i].unsqueeze(1)\n",
        "\n",
        "            else:\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.detach()\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions_weights = torch.stack(attentions_weights, dim=1)\n",
        "        return decoder_outputs, decoder_hidden, attentions_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d1e4b35",
      "metadata": {
        "id": "3d1e4b35"
      },
      "source": [
        "Create new encoder and decoders instances for this model, and train them !\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length, input_lang, output_lang):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Convert input sentence to tensor\n",
        "    input_tensor = tensorFromSentence(sentence, input_lang, max_length).view(1, -1).to(device)\n",
        "    # Forward pass through the encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    # Initialize the decoder\n",
        "    batch_size = encoder_hidden.size(1)\n",
        "    decoder_input = torch.full((batch_size, 1), SOS_TOKEN, dtype=torch.long, device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    decoded_words = []\n",
        "    eos_count = 0\n",
        "\n",
        "    # Forward pass through the decoder (attention weights returned if applicable)\n",
        "    decoder_outputs, decoder_hidden, decoder_attn = decoder(\n",
        "        encoder_outputs,\n",
        "        encoder_hidden,\n",
        "        max_length  # Ensure max_length is passed\n",
        "    )\n",
        "\n",
        "    # Decode output words\n",
        "    for t in range(max_length):\n",
        "        probs = F.softmax(decoder_outputs[:, t] / 2.0, dim=-1)\n",
        "        topi = torch.argmax(probs, dim=-1)\n",
        "        word_idx = topi.item()\n",
        "\n",
        "        if word_idx == EOS_TOKEN:\n",
        "            eos_count += 1\n",
        "            if eos_count >= 3:\n",
        "                break\n",
        "        else:\n",
        "            eos_count = 0\n",
        "\n",
        "        if word_idx in output_lang.idx2word:\n",
        "            decoded_words.append(output_lang.idx2word[word_idx])\n",
        "        else:\n",
        "            decoded_words.append(\"<UNK>\")  # 🔍 Debug placeholder for unknown words\n",
        "\n",
        "    # Return the decoded words and attention weights (ensure decoder_attn is a tensor)\n",
        "    return decoded_words, decoder_attn"
      ],
      "metadata": {
        "id": "YyPw7xrh60U6"
      },
      "id": "YyPw7xrh60U6",
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "c36774c7",
      "metadata": {
        "id": "c36774c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70db9e63-6aca-4f77-84e3-970d71e257aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4 5%) 3.2821\n",
            "(9 11%) 2.2467\n",
            "(14 17%) 1.6171\n",
            "(19 23%) 1.1380\n",
            "(24 30%) 0.8131\n",
            "(29 36%) 0.6003\n",
            "(34 42%) 0.4590\n",
            "(39 48%) 0.3594\n",
            "(44 55%) 0.3081\n",
            "(49 61%) 0.2707\n",
            "(54 67%) 0.2306\n",
            "(59 73%) 0.2228\n",
            "(64 80%) 0.2326\n",
            "(69 86%) 0.2146\n",
            "(74 92%) 0.1997\n",
            "(79 98%) 0.1908\n"
          ]
        }
      ],
      "source": [
        "encoder_att = EncoderRNN(input_size=len(training_dataset.input_lang),hidden_size=256).to(device)\n",
        "decoder_att = AttentionDecoderRNN(hidden_size=256, output_size=len(training_dataset.output_lang)).to(device)\n",
        "\n",
        "train(training_dataloader, encoder_att, decoder_att, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75d6278",
      "metadata": {
        "id": "a75d6278"
      },
      "source": [
        "Use the following function to visualize the attention learnt by the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "7ffe06a3",
      "metadata": {
        "id": "7ffe06a3"
      },
      "outputs": [],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().detach().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "    plt.show()\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence, encoder, decoder, dataset):\n",
        "    output_words, decoder_attn = evaluate(encoder, decoder, input_sentence, dataset.max_length, dataset.input_lang, dataset.output_lang)\n",
        "\n",
        "    # Check the dimensions of decoder_attn\n",
        "    if len(decoder_attn.shape) == 3:  # [batch=1, steps, seq_len]\n",
        "        showAttention(input_sentence, output_words, decoder_attn[0, :len(output_words), :])\n",
        "    else:\n",
        "        showAttention(input_sentence, output_words, decoder_attn[:len(output_words), :])\n",
        "\n",
        "    # Clean up output words\n",
        "    output_words = [word for word in output_words if word != \"<UNK>\"]\n",
        "    output_sentence = ' '.join(output_words)\n",
        "\n",
        "    # Remove \"je <UNK> <UNK>...\" patterns\n",
        "    output_sentence = re.sub(r'\\bje <UNK>( <UNK>)*\\b', '', output_sentence)\n",
        "    # Remove all standalone \"<UNK>\"\n",
        "    output_sentence = re.sub(r'\\s*<UNK>\\s*', ' ', output_sentence).strip()\n",
        "    # Remove \"je\" if it appears at the end of the sentence\n",
        "    output_sentence = re.sub(r'\\bje\\b[\\s.!?,;]*$', '', output_sentence).strip()\n",
        "    if output_words and output_words[-1] == \"je\":\n",
        "        output_words.pop()\n",
        "\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "0c0ddb1f",
      "metadata": {
        "id": "0c0ddb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fadfab1-58ad-4df2-dd74-048b585af7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> he is being very kind today.\n",
            "= il est très amical aujourd'hui.\n",
            "< il est très amical aujourd'hui.\n",
            "\n",
            "> you are what you are.\n",
            "= vous êtes comme vous êtes.\n",
            "< tu es comme tu es.\n",
            "\n",
            "> you are cleared for takeoff.\n",
            "= votre décollage est confirmé.\n",
            "< votre décollage est confirmé.\n",
            "\n",
            "> he is incapable of telling a lie.\n",
            "= il est incapable de mentir.\n",
            "< il est incapable de mentir.\n",
            "\n",
            "> you are taller than me.\n",
            "= vous êtes plus grand que moi.\n",
            "< vous es plus grands que moi.\n",
            "\n",
            "> she is trying to prove the existence of ghosts.\n",
            "= elle tente de prouver l'existence de fantômes.\n",
            "< elle tente de prouver l'existence de fantômes.\n",
            "\n",
            "> she is old.\n",
            "= elle est vieille.\n",
            "< elle est vieille.\n",
            "\n",
            "> she is the one who took care of his wound.\n",
            "= elle est celle qui a pris soin de sa blessure.\n",
            "< elle est celle qui a pris soin de sa blessure.\n",
            "\n",
            "> he is so careless that he often makes mistakes.\n",
            "= il est si insouciant qu'il commet souvent des erreurs.\n",
            "< il est si insouciant qu'il commet souvent des erreurs.\n",
            "\n",
            "> he is apt to forget.\n",
            "= il a tendance à oublier.\n",
            "< il a tendance à oublier.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder_att, decoder_att, training_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "3c992013",
      "metadata": {
        "id": "3c992013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "1c0ccae8-f7cd-4b3c-b6a1-53c76cb1711e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-112-a22a590fe5d5>:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
            "<ipython-input-112-a22a590fe5d5>:9: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_yticklabels([''] + output_words)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = i am not a doctor but a teacher \n",
            "output = je ne suis pas médecin, de un enseignant.\n"
          ]
        }
      ],
      "source": [
        "evaluateAndShowAttention('i am not a doctor but a teacher ', encoder_att, decoder_att, training_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03b0c870",
      "metadata": {
        "id": "03b0c870"
      },
      "source": [
        "We played here with a dataset but did not rigorously evaluate. The usual metric for Machine Translation is the [**BLEU Score**](https://aclanthology.org/P02-1040.pdf). You can find existing implementations, for example in [Huggingface](https://huggingface.co/spaces/evaluate-metric/bleu). Rigorously experiment with\n",
        "- A model with attention\n",
        "- A model without attention\n",
        "\n",
        "and use the BLEU score on the test set to compare them.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def evaluate_model_for_bleu(encoder, decoder, dataset, dataloader, device):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            input_tensor, target_tensor = data\n",
        "\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            target_tensor = target_tensor.to(device)\n",
        "\n",
        "            try:\n",
        "                batch_size = input_tensor.size(0)\n",
        "                max_length = input_tensor.size(1)\n",
        "\n",
        "                encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "                decoder_input = torch.full((batch_size, 1), SOS_TOKEN, dtype=torch.long, device=device)\n",
        "                decoder_hidden = encoder_hidden\n",
        "\n",
        "                decoder_outputs, _, _ = decoder(encoder_outputs, decoder_hidden, max_length)\n",
        "\n",
        "                for i in range(batch_size):\n",
        "                    ref_words = []\n",
        "                    for idx in target_tensor[i]:\n",
        "                        idx_item = idx.item()\n",
        "                        if idx_item == EOS_TOKEN:\n",
        "                            break\n",
        "                        if idx_item != PAD_TOKEN and idx_item in dataset.output_lang.idx2word:\n",
        "                            ref_words.append(dataset.output_lang.idx2word[idx_item])\n",
        "\n",
        "                    if len(ref_words) > 0:\n",
        "                        references.append([ref_words])\n",
        "\n",
        "                        hyp_words = []\n",
        "                        for t in range(max_length):\n",
        "                            topv, topi = decoder_outputs[i, t].topk(1)\n",
        "                            word_idx = topi.cpu().item()\n",
        "\n",
        "                            if word_idx == EOS_TOKEN:\n",
        "                                break\n",
        "                            if word_idx != PAD_TOKEN and word_idx in dataset.output_lang.idx2word:\n",
        "                                hyp_words.append(dataset.output_lang.idx2word[word_idx])\n",
        "\n",
        "                        hypotheses.append([w for w in hyp_words if w not in ['<UNK>', '<PAD>', '<EOS>']])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing batch: {e}\")\n",
        "                continue\n",
        "\n",
        "    if len(references) == 0 or len(hypotheses) == 0:\n",
        "        return {'bleu1': 0, 'bleu2': 0, 'bleu3': 0, 'bleu4': 0}\n",
        "\n",
        "    smoothie = SmoothingFunction().method1\n",
        "\n",
        "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
        "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
        "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
        "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
        "\n",
        "    return {\n",
        "        'bleu1': bleu1 * 100,\n",
        "        'bleu2': bleu2 * 100,\n",
        "        'bleu3': bleu3 * 100,\n",
        "        'bleu4': bleu4 * 100\n",
        "    }\n",
        "\n",
        "def compare_models_with_bleu():\n",
        "    print(\"==== Comparison about BLEU scores ====\")\n",
        "\n",
        "    print(\"\\n1. No Attention...\")\n",
        "    try:\n",
        "        no_attention_scores = evaluate_model_for_bleu(encoder, decoder, training_dataset, test_dataloader, device)\n",
        "        print(f\"No Attention BLEU-1: {no_attention_scores['bleu1']:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in No attention: {e}\")\n",
        "        no_attention_scores = {'bleu1': 0, 'bleu2': 0, 'bleu3': 0, 'bleu4': 0}\n",
        "\n",
        "    print(\"\\n2. With Attention...\")\n",
        "    try:\n",
        "        with_attention_scores = evaluate_model_for_bleu(encoder_att, decoder_att, training_dataset, test_dataloader, device)\n",
        "        print(f\"With Attention BLEU-1: {with_attention_scores['bleu1']:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in With attention: {e}\")\n",
        "        with_attention_scores = {'bleu1': 0, 'bleu2': 0, 'bleu3': 0, 'bleu4': 0}\n",
        "\n",
        "    print(\"\\n==== Comparison about BLEU scores(%) ====\")\n",
        "    print(f\"{'Model':<20} {'BLEU-1':<10} {'BLEU-2':<10} {'BLEU-3':<10} {'BLEU-4':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'No Attention':<20} {no_attention_scores['bleu1']:<10.2f} {no_attention_scores['bleu2']:<10.2f} {no_attention_scores['bleu3']:<10.2f} {no_attention_scores['bleu4']:<10.2f}\")\n",
        "    print(f\"{'With Attention':<20} {with_attention_scores['bleu1']:<10.2f} {with_attention_scores['bleu2']:<10.2f} {with_attention_scores['bleu3']:<10.2f} {with_attention_scores['bleu4']:<10.2f}\")\n",
        "\n",
        "    improvements = {}\n",
        "    for key in no_attention_scores:\n",
        "        if no_attention_scores[key] > 0:\n",
        "            improvements[key] = ((with_attention_scores[key] - no_attention_scores[key]) / no_attention_scores[key]) * 100\n",
        "        else:\n",
        "            improvements[key] = float('inf')\n",
        "\n",
        "    print(\"\\n==== Improvements by Attention ====\")\n",
        "    for key, value in improvements.items():\n",
        "        print(f\"{key}: {value:+.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'no_attention': no_attention_scores,\n",
        "        'with_attention': with_attention_scores,\n",
        "        'improvements': improvements\n",
        "    }\n",
        "\n",
        "\n",
        "try:\n",
        "    results = compare_models_with_bleu()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Try again on cpu...\")\n",
        "\n",
        "    encoder.to('cpu')\n",
        "    decoder.to('cpu')\n",
        "    encoder_att.to('cpu')\n",
        "    decoder_att.to('cpu')\n",
        "\n",
        "    results = compare_models_with_bleu()\n",
        "\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "    encoder_att.to(device)\n",
        "    decoder_att.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ngadFq--KXk",
        "outputId": "d6117f04-0148-494d-bed8-8c287eb53850"
      },
      "id": "4ngadFq--KXk",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Comparison about BLEU scores ====\n",
            "\n",
            "1. No Attention...\n",
            "No Attention BLEU-1: 44.72\n",
            "\n",
            "2. With Attention...\n",
            "With Attention BLEU-1: 46.04\n",
            "\n",
            "==== Comparison about BLEU scores(%) ====\n",
            "Model                BLEU-1     BLEU-2     BLEU-3     BLEU-4    \n",
            "------------------------------------------------------------\n",
            "No Attention         44.72      36.32      28.97      22.96     \n",
            "With Attention       46.04      35.68      27.81      22.16     \n",
            "\n",
            "==== Improvements by Attention ====\n",
            "bleu1: +2.94%\n",
            "bleu2: -1.78%\n",
            "bleu3: -4.00%\n",
            "bleu4: -3.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b956da",
      "metadata": {
        "id": "14b956da"
      },
      "source": [
        "We improved our initial model with attention. But considering our goal is to **generate text**, we should work on **decoding**. How would you go about implementing that given our current code ? Where is the ideal place to add a decoding function ?\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d715ae44",
      "metadata": {
        "id": "d715ae44"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "78018765",
      "metadata": {
        "id": "78018765"
      },
      "source": [
        "Propose a modification of the *relevant function* in this lab in which to include **Beam search**, following the code you used in the previous lab. Similarly as before, compare the BLEU score of:\n",
        "- A model decoding with Beam search\n",
        "- A model using simple greedy decoding\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def greedy_decode(encoder, decoder, sentence, input_lang, output_lang, max_length):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    input_tensor = tensorFromSentence(sentence, input_lang, max_length).unsqueeze(0).to(device)\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoded_words = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        decoder_output, decoder_hidden = decoder.forward_step(decoder_input, decoder_hidden)\n",
        "\n",
        "        topv, topi = decoder_output.squeeze(1).topk(1)\n",
        "        word_idx = topi.item()\n",
        "\n",
        "        if word_idx == EOS_TOKEN:\n",
        "            break\n",
        "        decoded_words.append(output_lang.idx2word[word_idx])\n",
        "\n",
        "        decoder_input = topi.detach().view(1, 1)\n",
        "\n",
        "    return decoded_words\n",
        "\n",
        "def beam_search_decode(encoder, decoder, sentence, input_lang, output_lang, max_length, beam_width=3):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    input_tensor = tensorFromSentence(sentence, input_lang, max_length).unsqueeze(0).to(device)\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "    beams = [(torch.tensor([[SOS_TOKEN]], device=device), 0, encoder_hidden)]\n",
        "    completed_sequences = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        new_beams = []\n",
        "        for seq, score, hidden in beams:\n",
        "            decoder_input = seq[:, -1].view(1, 1)\n",
        "\n",
        "            decoder_output, hidden = decoder.forward_step(decoder_input, hidden)\n",
        "\n",
        "            topv, topi = decoder_output.squeeze(1).topk(beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                word_idx = topi[0][i].item()\n",
        "                word_score = topv[0][i].item()\n",
        "\n",
        "                new_seq = torch.cat([seq, torch.tensor([[word_idx]], device=device)], dim=1)\n",
        "                new_score = score + word_score\n",
        "\n",
        "                if word_idx == EOS_TOKEN:\n",
        "                    completed_sequences.append((new_seq, new_score))\n",
        "                else:\n",
        "                    new_beams.append((new_seq, new_score, hidden))\n",
        "\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        if len(completed_sequences) >= beam_width:\n",
        "            break\n",
        "\n",
        "    if len(completed_sequences) > 0:\n",
        "        best_sequence, _ = max(completed_sequences, key=lambda x: x[1])\n",
        "    else:\n",
        "        best_sequence = beams[0][0]\n",
        "\n",
        "    decoded_words = [output_lang.idx2word[idx.item()] for idx in best_sequence.squeeze() if idx.item() not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]]\n",
        "\n",
        "    return decoded_words\n",
        "\n",
        "def evaluate_bleu_on_test(encoder, decoder, test_dataloader, dataset, method=\"greedy\", beam_width=3):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_tensor, target_tensor in tqdm(test_dataloader):\n",
        "            input_texts = [\" \".join([dataset.input_lang.idx2word[idx.item()]\n",
        "                                     for idx in input_tensor[i] if idx.item() not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]])\n",
        "                           for i in range(len(input_tensor))]\n",
        "\n",
        "            target_texts = [\" \".join([dataset.output_lang.idx2word[idx.item()]\n",
        "                                      for idx in target_tensor[i] if idx.item() not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]])\n",
        "                            for i in range(len(target_tensor))]\n",
        "\n",
        "            for i, input_sentence in enumerate(input_texts):\n",
        "                if method == \"greedy\":\n",
        "                    output_words = greedy_decode(encoder, decoder, input_sentence, dataset.input_lang, dataset.output_lang, dataset.max_length)\n",
        "                elif method == \"beam\":\n",
        "                    output_words = beam_search_decode(encoder, decoder, input_sentence, dataset.input_lang, dataset.output_lang, dataset.max_length, beam_width)\n",
        "\n",
        "                references.append([target_texts[i].split()])\n",
        "                hypotheses.append(output_words)\n",
        "\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
        "    return bleu_score * 100\n",
        "\n",
        "bleu_greedy_test = evaluate_bleu_on_test(encoder, decoder, test_dataloader, training_dataset, method=\"greedy\")\n",
        "bleu_beam_test = evaluate_bleu_on_test(encoder, decoder, test_dataloader, training_dataset, method=\"beam\", beam_width=3)\n",
        "\n",
        "print(f\"BLEU Score on Test Set (Greedy Decoding): {bleu_greedy_test:.2f}\")\n",
        "print(f\"BLEU Score on Test Set (Beam Search Decoding): {bleu_beam_test:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p6d7TZJIVoL",
        "outputId": "c0e33c39-ec51-4656-9f15-183a5bd6e87b"
      },
      "id": "-p6d7TZJIVoL",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.11it/s]\n",
            "100%|██████████| 12/12 [00:05<00:00,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score on Test Set (Greedy Decoding): 17.95\n",
            "BLEU Score on Test Set (Beam Search Decoding): 15.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59ce85d0",
      "metadata": {
        "id": "59ce85d0"
      },
      "source": [
        "Find a model on [Huggingface](https://huggingface.co/tasks/translation) for this task. Try to **understand what model it is** - and who trained it, on which data. Apply it to the same dataset and compute the BLEU score.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "def translate_with_huggingface(sentence):\n",
        "\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    translated = model.generate(**inputs, max_length=50)\n",
        "    output = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "    return output\n",
        "\n",
        "def evaluate_huggingface_bleu(test_dataloader, dataset):\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    print(\"Translating test set and computing BLEU score...\")\n",
        "\n",
        "    for input_tensor, target_tensor in tqdm(test_dataloader):\n",
        "\n",
        "        input_texts = [dataset.input_lang.idx2word[idx.item()] for idx in input_tensor[0] if idx.item() not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]]\n",
        "        target_texts = [dataset.output_lang.idx2word[idx.item()] for idx in target_tensor[0] if idx.item() not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]]\n",
        "\n",
        "        input_sentence = \" \".join(input_texts)\n",
        "        target_sentence = \" \".join(target_texts)\n",
        "\n",
        "        output_sentence = translate_with_huggingface(input_sentence)\n",
        "\n",
        "        references.append([target_sentence.split()])\n",
        "        hypotheses.append(output_sentence.split())\n",
        "\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
        "    return bleu_score * 100\n",
        "\n",
        "\n",
        "bleu_huggingface = evaluate_huggingface_bleu(test_dataloader, training_dataset)\n",
        "print(f\"BLEU Score on Test Set (Hugging Face Model - {model_name}): {bleu_huggingface:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqNsSr0YJSV5",
        "outputId": "abe4b421-c8c1-4a72-d545-be0282cdcceb"
      },
      "id": "BqNsSr0YJSV5",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translating test set and computing BLEU score...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 10.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score on Test Set (Hugging Face Model - Helsinki-NLP/opus-mt-en-fr): 37.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}